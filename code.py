# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KOlSl8NIWN4L6uqgCDCLh1bCVtAmBJdj
"""

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# 1C8dVqqx3VCw5WABfaAIE4Y3bNruc3vV_ bd_sentences

downloaded = drive.CreateFile({'id':"1L8WYkM0D6XBBaoJQjVrGLTb93vouk9bO"})   # replace the id with id of file you want to access
downloaded.GetContentFile('infos.zip') 
!unzip infos.zip

import pickle,os,re
word2index=pickle.load( open( "word2indexUpdated_100.p", "rb" ) )
index2word=pickle.load( open( "index2wordUpdated_100.p", "rb" ) )
currentCount=len(word2index)
vocab_size=currentCount+1
from keras.callbacks import ModelCheckpoint
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional
seq_length=100
batch_size=128
model=Sequential()
model.add(Embedding(vocab_size, 10, input_length=seq_length,mask_zero=True))
model.add(Bidirectional(LSTM(10)))
model.add(Dense(5, activation='relu'))
model.add(Dense(vocab_size, activation='softmax'))
def getTokenizedSentences(rows,word2index):
    sentences=[]
    for r in rows:
        if len(r)>1:
            words=r.split(" ")
            sentence=[]
            for word in words:
                if word in word2index:
                    sentence.append(word2index[word])
            sentences.append(sentence)
    return sentences
def getTokenizedSentencesWords(rows):
    sentences=[]
    for r in rows:
        if len(r)>1:
            words=r.split(" ")
            sentences+=words
    return sentences
def updateSentenceLengthWise(tokenizedSentences,maxLen=100):
    final_sentences=[]
    for s in tokenizedSentences:
        if len(s)<maxLen:
            final_sentences.append(s)
        else:
            chunks = [s[x:x+maxLen] for x in range(0, len(s), maxLen)]
            final_sentences+=chunks
    return final_sentences
def getTrainableData(tokenizedSentences):
    X,Y=[],[]
    for sentence in tokenizedSentences:
        for i in range(len(sentence)):
            y_label=sentence[i]
            new_s=sentence[:]
            new_s[i]=1  
            X.append(new_s)
            Y.append(y_label)
    return X,Y
def data_generator(sentences, batch_size = 64):
    
    while True:
        # Select files (paths/indices) for the batch
        batch_sentences = np.random.choice(a = sentences, 
                                     size = batch_size)
        X,Y=getTrainableData(batch_sentences)
        X=pad_sequences(X,dtype='int32', padding='post', value=0,maxlen=seq_length)
        Y = to_categorical(Y, num_classes=vocab_size)   
        yield( X,Y )
filepath="weights/bs-bangla-3-{epoch:02d}-{acc:.2f}.h5"
checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True, mode='max',save_weights_only=False)
callbacks_list = [checkpoint]
import os
if not os.path.exists("weights"):
    os.makedirs("weights")

bd_data=pickle.load( open( "infos/book_sentences.p", "rb" ) )
bd_data=bd_data[1500000:]
batch_size=64
bd_tokenized_sentences=getTokenizedSentences(bd_data,word2index)
final_sentences=updateSentenceLengthWise(bd_tokenized_sentences)
steps_per_epoch=len(final_sentences)//batch_size
training_generator = data_generator(final_sentences,batch_size=batch_size)
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.load_weights("bs-bangla-2-04-0.08.h5")
model.fit_generator(generator=training_generator,
                    callbacks=callbacks_list,epochs=30,steps_per_epoch=steps_per_epoch)







word2index=pickle.load( open( "finalWords_100.p", "rb" ) )
# index2word=pickle.load( open( "index2wordUpdated.p", "rb" ) )
currentCount=len(word2index)

word2indexUpdated,index2wordUpdated=dict(),dict()
for i,word in enumerate(word2index):
    word2indexUpdated[word]=i+2
    index2wordUpdated[i+2]=word
word2indexUpdated["<UNK>"]=1
index2wordUpdated[1]="<UNK>"

pickle.dump( word2indexUpdated, open( "word2indexUpdated_100.p", "wb" ) )
pickle.dump( index2wordUpdated, open( "index2wordUpdated_100.p", "wb" ) )

currentCount

def getTokenizedSentences(rows,word2index):
    sentences=[]
    for r in rows:
        if len(r)>1:
            words=r.split(" ")
            sentence=[]
            for word in words:
                if word in word2index:
                    sentence.append(word2index[word])
            sentences.append(sentence)
    return sentences

bd_data=pickle.load( open( "infos/book_sentences.p", "rb" ) )

len(bd_data)

bd_data[:10]

vocab_size=currentCount+1

vocab_size

from keras.callbacks import ModelCheckpoint
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional

filepath="weights/bs-bangla-{epoch:02d}-{acc:.2f}.h5"
checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True, mode='max',save_weights_only=False)
callbacks_list = [checkpoint]

seq_length=100
batch_size=128

model=Sequential()
model.add(Embedding(vocab_size, 50, input_length=seq_length,mask_zero=True))
model.add(Bidirectional(LSTM(150,return_sequences=True)))
model.add(Bidirectional(LSTM(120)))
model.add(Dense(50, activation='relu'))
model.add(Dense(vocab_size, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

# serialize model to JSON
model_json = model.to_json()
with open("bs-model.json", "w") as json_file:
    json_file.write(model_json)

# model.load_weights("weights/bs-bd-12-02-0.77.h5")

def updateSentenceLengthWise(tokenizedSentences,maxLen=100):
    final_sentences=[]
    for s in tokenizedSentences:
        if len(s)<maxLen:
            final_sentences.append(s)
        else:
            chunks = [s[x:x+maxLen] for x in range(0, len(s), maxLen)]
            final_sentences+=chunks
    return final_sentences

def getTrainableData(tokenizedSentences):
    X,Y=[],[]
    for sentence in tokenizedSentences:
        for i in range(len(sentence)):
            y_label=sentence[i]
            new_s=sentence[:]
            new_s[i]=1  
            X.append(new_s)
            Y.append(y_label)
    return X,Y

def data_generator(sentences, batch_size = 64):
    
    while True:
        # Select files (paths/indices) for the batch
        batch_sentences = np.random.choice(a = sentences, 
                                     size = batch_size)
        X,Y=getTrainableData(batch_sentences)
        X=pad_sequences(X,dtype='int32', padding='post', value=0,maxlen=seq_length)
        Y = to_categorical(Y, num_classes=vocab_size)   
        yield( X,Y )

bd_data=bd_data[:500000]

batch_size=64
bd_tokenized_sentences=getTokenizedSentences(bd_data,word2indexUpdated)
final_sentences=updateSentenceLengthWise(bd_tokenized_sentences)
steps_per_epoch=len(final_sentences)//batch_size

len(final_sentences)

final_sentences[:5]

training_generator = data_generator(final_sentences,batch_size=batch_size)

import os
if not os.path.exists("weights"):
    os.makedirs("weights")

model.fit_generator(generator=training_generator,
                    callbacks=callbacks_list,epochs=10,steps_per_epoch=steps_per_epoch)

test_data=bd_data[2:5]

test_data

tok_s=getTokenizedSentences(test_data,word2indexUpdated)

tok_s

batch_sentences = np.random.choice(a = tok_s, 
                                     size = 2)

batch_sentences

X,Y=getTrainableData(batch_sentences)

a=data_generator(tok_s,batch_size=2)

x,y=next(a)

y.shape

np.argmax(y[1])

tok_s[0][1]=1

up_sl=pad_sequences(tok_s,dtype='int32', padding='post', value=0,maxlen=seq_length)

up_sl

pred_y=model.predict(up_sl)

np.argmax(pred_y[0])

for subdir, dirs, files in os.walk("prothom_alo/"):
    for file in files:
        if ".txt" in file.lower():
            filePath=os.path.join(subdir,file)
            print(filePath)
            all_tokenized_sentences=[]
            with open (filePath, 'r', encoding="utf-8") as f:
                row=f.readlines()
#                 all_sentences+=getAllSentencesFromRows(row)
                all_tokenized_sentences+=getTokenizedSentences(row)
            pickle.dump( all_tokenized_sentences, open( "prothom_alo_tokenized/"+file+".p", "wb" ) )
            # dataFrame   = pd.DataFrame(data=all_tokenized_sentences)
            # dataFrame.to_feather(filePath+".ftr")
            del all_tokenized_sentences

!zip prothom_alo_token_3.zip prothom_alo_tokenized/prothom-alo-n-2018.txt.p prothom_alo_tokenized/prothom-alo-n-2019.txt.p

